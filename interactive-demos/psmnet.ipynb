{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0xqQCHhrKx-"
      },
      "source": [
        "# Pyramid Stereo Matching Network\n",
        "Input | Output\n",
        "--- | ---\n",
        "![alt text](https://github.com/sony/nnabla-examples/raw/master/stereo-depth/PSMnet/results/0006.png) | ![alt text](https://github.com/sony/nnabla-examples/raw/master/stereo-depth/PSMnet/results/depth_sceneflow.png)\n",
        "![alt text](https://github.com/sony/nnabla-examples/raw/master/stereo-depth/PSMnet/results/000000_10.png) | ![alt text](https://github.com/sony/nnabla-examples/raw/master/stereo-depth/PSMnet/results/depth_kitti.png)\n",
        "\n",
        "This example interactively demonstrates [Pyramid Stereo Matching Network](https://arxiv.org/abs/1803.08669), a model for depth estimation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dv8cjithm7H"
      },
      "source": [
        "# Preparation\n",
        "Let's start by installing nnabla and accessing [nnabla-examples repository](https://github.com/sony/nnabla-examples). If you're running on Colab, make sure that your Runtime setting is set as GPU, which can be set up from the top menu (Runtime â†’ change runtime type), and make sure to click **Connect** on the top right-hand side of the screen before you start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7GDBqHkhm7I"
      },
      "outputs": [],
      "source": [
        "# May show warnings for newly imported packages if run in Colab default python environment.\n",
        "# Please click the `RESTART RUNTIME` to run the following script correctly.\n",
        "# The error message of conflicts is acceptable.\n",
        "!pip install nnabla-ext-cuda116\n",
        "!git clone https://github.com/sony/nnabla-examples.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0boUKYrthm7J"
      },
      "outputs": [],
      "source": [
        "%cd nnabla-examples/stereo-depth/PSMnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-IlFSeWhm7J"
      },
      "source": [
        "We need to download the pretrained weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV4Z6Ahlhm7K"
      },
      "outputs": [],
      "source": [
        "# get PSMNet pretrained weights trained on Scene Flow Datasets.\n",
        "!wget https://nnabla.org/pretrained-models/nnabla-examples/stereo-depth/psmnet_trained_sceneflow.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spMhT0bEhm7K"
      },
      "source": [
        "# Get images for inference\n",
        "To use PSMNet, you need stereo vision images, in other words, **you need 2 images**, one is the left view, and the other is the right view. In this demo, we use images from [Scene Flow dataset](k.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html) on which PSMNet is trained. First you need to download the dataset, but since the entire dataset is too large, we use the Example pack for instance. After downloading dataset, decompress the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCSvWNTIhm7L"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlow/assets/Sampler.tar.gz\n",
        "!tar -xvf Sampler.tar.gz > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBxE7YHDhm7M"
      },
      "source": [
        "# Choose a left image\n",
        "\n",
        "Run the below cell to choose a left view image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGx4Szg7hm7M"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import ipywidgets as widgets\n",
        "choices = sorted(glob.glob('Sampler/**/*.png', recursive=True))\n",
        "left_images = [_ for _ in choices if \"/left/\" in _]\n",
        "right_images = [_ for _ in choices if \"/right/\" in _]\n",
        "left_image_path = widgets.Dropdown(options=left_images, value=left_images[0])\n",
        "left_image_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi1Cjj_4hm7M"
      },
      "source": [
        "Next is the right view image. Running the following cell automatically finds the right view image corresponding to the left one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emoh204chm7M"
      },
      "outputs": [],
      "source": [
        "left_image_path = left_image_path.get_interact_value()\n",
        "right_image_path = left_image_path.replace(\"/left/\", \"/right/\")\n",
        "assert right_image_path in right_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkjszMkIhm7N"
      },
      "source": [
        "Let's take a look at the chosen image (Here the left image is displayed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8FT6jHYhm7N"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(left_image_path, width=960/2., height=512/2.))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8ohPWVwhm7N"
      },
      "source": [
        "# Run PSMNet for depth estimation\n",
        "\n",
        "Now that we have input stereo paired images, We're ready to run inference script! Just execute the following cell to make it run (It may take time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbYKATTThm7N"
      },
      "outputs": [],
      "source": [
        "!python inference.py --dataset SceneFlow --loadmodel ./psmnet_trained_sceneflow.h5 --save-nnp False -l $left_image_path -r $right_image_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8XDvOF0hm7N"
      },
      "source": [
        "Once the inference is done, you should get the result image named \"stereo_depth.png\". Let's see the result and compare it with the input image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg8O84nchm7N"
      },
      "outputs": [],
      "source": [
        "display(Image('stereo_depth.png', width=960/2., height=512/2.))\n",
        "display(Image(left_image_path, width=960/2., height=512/2.))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}